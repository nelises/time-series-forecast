{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages & Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from dateutil.parser import parse\n",
    "import re\n",
    "import webbrowser\n",
    "from ydata_profiling import ProfileReport\n",
    "from ydata_profiling.config import Settings\n",
    "\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "import scipy.stats as stats\n",
    "\n",
    "import gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show graphs\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Drive file \n",
    "file_id = '1yvAWHeK86zTHyQvJi0paT3-EP19JYk0L'\n",
    "# Create the download URL\n",
    "download_url = f'https://drive.google.com/uc?export=download&id={file_id}'\n",
    "\n",
    "# Download the file \n",
    "output_path = 'unemployment_dataset.csv'\n",
    "gdown.download(download_url, output_path, quiet=False)\n",
    "\n",
    "# Load the CSV \n",
    "unemployment_df = pd.read_csv(output_path)\n",
    "\n",
    "# Display the DataFrame \n",
    "print(unemployment_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Drive file \n",
    "file_id = '1bASVHvEJkKQCwamT3NpqVNxIe4tde5ke'\n",
    "# Create the download URL\n",
    "download_url = f'https://drive.google.com/uc?export=download&id={file_id}'\n",
    "\n",
    "# Download the file \n",
    "output_path = 'wage_dataset.csv'\n",
    "gdown.download(download_url, output_path, quiet=False)\n",
    "\n",
    "# Load the CSV \n",
    "wage_df = pd.read_csv(output_path)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(wage_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check and drop unnecessary columns\n",
    "columns_to_drop = ['DGUID', 'Type of work', 'UOM', 'UOM_ID', 'SCALAR_FACTOR', 'SCALAR_ID', 'VECTOR', 'COORDINATE', 'STATUS', 'SYMBOL', 'TERMINATED', 'DECIMALS']\n",
    "columns_to_drop_unemployment = [col for col in columns_to_drop if col in unemployment_df.columns]\n",
    "columns_to_drop_wage = [col for col in columns_to_drop if col in wage_df.columns]\n",
    "\n",
    "data_employment = unemployment_df.drop(columns=columns_to_drop_unemployment)\n",
    "data_wage = wage_df.drop(columns=columns_to_drop_wage)\n",
    "\n",
    "# Ensure column names are normalized (lowercase and replace spaces with underscores)\n",
    "data_employment.columns = [col.lower().replace(' ', '_') for col in data_employment.columns]\n",
    "data_wage.columns = [col.lower().replace(' ', '_') for col in data_wage.columns]\n",
    "\n",
    "# Rename the column from 'north_american_industry_classification_system_(naics)' to 'industry'\n",
    "data_employment.rename(columns={'north_american_industry_classification_system_(naics)': 'industry'}, inplace=True)\n",
    "data_wage.rename(columns={'north_american_industry_classification_system_(naics)': 'industry'}, inplace=True)\n",
    "\n",
    "# Rename the column from 'labour_force_characteristics' to 'labour_force'\n",
    "data_employment.rename(columns={'labour_force_characteristics': 'labour_force'}, inplace=True)\n",
    "data_wage.rename(columns={'labour_force_characteristics': 'labour_force'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Date standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wage - date standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fix inconsistent date formats\n",
    "def fix_date_format(date_string):\n",
    "    if pd.isna(date_string):\n",
    "        return pd.NaT\n",
    "\n",
    "    # Strip any leading/trailing spaces\n",
    "    date_string = date_string.strip()\n",
    "\n",
    "    # Check if date is in 'YY-MMM' or 'MMM-YY' format and convert\n",
    "    # Handle 'YY-MMM' (e.g., '21-Mar') and 'MMM-YY' (e.g., 'Mar-21')\n",
    "    if re.match(r'^\\d{2}-[A-Za-z]{3}$', date_string):\n",
    "        year, month = date_string.split('-')\n",
    "        date_string = f'{month}-{year}'\n",
    "    elif re.match(r'^[A-Za-z]{3}-\\d{2}$', date_string):\n",
    "        month, year = date_string.split('-')\n",
    "        # Correctly handle century assumption based on year value\n",
    "        if int(year) >= 97:  # If year is 97, 98, 99 -> assume 1997, 1998, 1999\n",
    "            date_string = f'{month}-19{year}'\n",
    "        else:  # If year is 00-23 -> assume 2000s\n",
    "            date_string = f'{month}-20{year}'\n",
    "\n",
    "    # Try parsing the date using dateutil\n",
    "    try:\n",
    "        parsed_date = parse(date_string, fuzzy=False)\n",
    "        # Set the day explicitly to the 1st of the month if not already provided\n",
    "        parsed_date = parsed_date.replace(day=1)\n",
    "        return parsed_date\n",
    "    except ValueError:\n",
    "        return pd.NaT\n",
    "\n",
    "# Apply the function to the 'ref_date' column to standardize the format\n",
    "data_wage['ref_date'] = data_wage['ref_date'].apply(fix_date_format)\n",
    "\n",
    "# Convert to the final 'yyyy-mm-dd' format\n",
    "data_wage['ref_date'] = data_wage['ref_date'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# Print the updated DataFrame to verify the results\n",
    "print(\"\\nUpdated DataFrame with 'ref_date' in 'yyyy-mm-dd' format:\")\n",
    "print(data_wage.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of missing values in the 'ref_date' column\n",
    "missing_values_count = data_wage['ref_date'].isna().sum()\n",
    "\n",
    "# Print the count of missing values\n",
    "print(f\"Number of missing values in 'ref_date': {missing_values_count}\")\n",
    "\n",
    "# Filter and list all rows where 'ref_date' is missing\n",
    "missing_rows = data_wage[data_wage['ref_date'].isna()]\n",
    "\n",
    "# Print the rows with missing 'ref_date' values\n",
    "print(\"\\nRows with missing 'ref_date':\")\n",
    "print(missing_rows)\n",
    "\n",
    "print(data_wage.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to verify if all values in date format 'yyyy-mm-dd'\n",
    "def is_valid_yyyy_mm_dd_format(date_string):\n",
    "    try:\n",
    "        # Try to parse using the expected format\n",
    "        datetime_obj = pd.to_datetime(date_string, format='%Y-%m-%d', errors='coerce')\n",
    "        return not pd.isna(datetime_obj)  \n",
    "    except:\n",
    "        return False\n",
    "\n",
    "# Rechecking each value in the 'ref_date' column to ensure they are properly formatted\n",
    "invalid_date_rows = []\n",
    "\n",
    "for idx, date in enumerate(data_wage['ref_date']):\n",
    "    if pd.isna(date) or is_valid_yyyy_mm_dd_format(date):\n",
    "        continue\n",
    "    else:\n",
    "        # If the date does not match 'yyyy-mm-dd', add the row to invalid list\n",
    "        invalid_date_rows.append(data_wage.iloc[idx])\n",
    "\n",
    "# Print the results of the validation\n",
    "if len(invalid_date_rows) == 0:\n",
    "    print(\"All dates in 'ref_date' are properly formatted in 'yyyy-mm-dd'.\")\n",
    "else:\n",
    "    print(\"\\nRows with incorrectly formatted 'ref_date':\")\n",
    "    for row in invalid_date_rows:\n",
    "        print(row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unemployment - date standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fix inconsistent date formats\n",
    "def fix_date_format_emp(date_string_emp):\n",
    "    if pd.isna(date_string_emp):\n",
    "        return pd.NaT\n",
    "\n",
    "    # Strip any spaces\n",
    "    date_string_emp = date_string_emp.strip()\n",
    "\n",
    "    # Check if date is in 'YY-MMM' or 'MMM-YY' format and convert\n",
    "    # Handle 'YY-MMM' (e.g., '21-Mar') and 'MMM-YY' (e.g., 'Mar-21')\n",
    "    if re.match(r'^\\d{2}-[A-Za-z]{3}$', date_string_emp):\n",
    "        year, month = date_string_emp.split('-')\n",
    "        date_string_emp = f'{month}-{year}'\n",
    "    elif re.match(r'^[A-Za-z]{3}-\\d{2}$', date_string_emp):\n",
    "        month, year = date_string_emp.split('-')\n",
    "        # Correctly handle century assumption based on year value\n",
    "        if int(year) >= 97:  # If year is >= 97 -> assume 1997, 1998, 1999\n",
    "            date_string_emp = f'{month}-19{year}'\n",
    "        else:  # If year is 00-23 -> assume 2000s\n",
    "            date_string_emp = f'{month}-20{year}'\n",
    "\n",
    "    # Try parsing the date using dateutil\n",
    "    try:\n",
    "        parsed_date_emp = parse(date_string_emp, fuzzy=False)\n",
    "        # Set the day explicitly to the 1st of the month \n",
    "        parsed_date_emp = parsed_date_emp.replace(day=1)\n",
    "        return parsed_date_emp\n",
    "    except ValueError:\n",
    "        return pd.NaT\n",
    "\n",
    "# Apply the function to the 'ref_date' column to standardize the format\n",
    "data_employment['ref_date'] = data_employment['ref_date'].apply(fix_date_format_emp)\n",
    "\n",
    "# Convert to the final 'yyyy-mm-dd' format\n",
    "data_employment['ref_date'] = data_employment['ref_date'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# Print the updated DataFrame to verify the results\n",
    "print(\"\\nUpdated DataFrame with 'ref_date' in 'yyyy-mm-dd' format:\")\n",
    "print(data_employment.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of missing values in the 'ref_date' column\n",
    "missing_values_count_emp = data_employment['ref_date'].isna().sum()\n",
    "\n",
    "# Print the count of missing values\n",
    "print(f\"Number of missing values in 'ref_date': {missing_values_count_emp}\")\n",
    "\n",
    "# Filter and list all rows where 'ref_date' is missing\n",
    "missing_rows_emp = data_employment[data_employment['ref_date'].isna()]\n",
    "\n",
    "# Print the rows with missing 'ref_date' values\n",
    "print(\"\\nRows with missing 'ref_date':\")\n",
    "print(missing_rows_emp)\n",
    "\n",
    "print(data_employment.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to verify if all values in date format 'yyyy-mm-dd'\n",
    "def is_valid_yyyy_mm_dd_format_emp(date_string_emp):\n",
    "    try:\n",
    "        # Try to parse using the expected format\n",
    "        datetime_obj_emp = pd.to_datetime(date_string_emp, format='%Y-%m-%d', errors='coerce')\n",
    "        return not pd.isna(datetime_obj_emp)  \n",
    "    except:\n",
    "        return False\n",
    "\n",
    "# Rechecking each value in the 'ref_date' column to ensure they are properly formatted\n",
    "invalid_date_rows_emp = []\n",
    "\n",
    "for idx, date in enumerate(data_employment['ref_date']):\n",
    "    if pd.isna(date) or is_valid_yyyy_mm_dd_format_emp(date):\n",
    "        continue\n",
    "    else:\n",
    "        # If the date does not match 'yyyy-mm-dd', add the row to invalid list\n",
    "        invalid_date_rows_emp.append(data_employment.iloc[idx])\n",
    "\n",
    "# Print the results of the validation\n",
    "if len(invalid_date_rows_emp) == 0:\n",
    "    print(\"All dates in 'ref_date' are properly formatted in 'yyyy-mm-dd'.\")\n",
    "else:\n",
    "    print(\"\\nRows with incorrectly formatted 'ref_date':\")\n",
    "    for row in invalid_date_rows_emp:\n",
    "        print(row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_employment.columns)\n",
    "print(data_wage.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge datasets on common columns\n",
    "merged_df = pd.merge(data_employment, data_wage, \n",
    "                     on=[\"ref_date\", \"geo\", \"industry\", \"sex\", \"age_group\"],\n",
    "                     how=\"outer\",\n",
    "                     suffixes=('_unemployment', '_wage'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'ref_date' to datetime format\n",
    "merged_df['ref_date'] = pd.to_datetime(merged_df['ref_date'], errors='coerce')\n",
    "\n",
    "# Extract year and month from 'ref_date' for correlation analysis\n",
    "merged_df['year'] = merged_df['ref_date'].dt.year\n",
    "merged_df['month'] = merged_df['ref_date'].dt.month\n",
    "\n",
    "# convert 'ref_date' to a numeric timestamp\n",
    "merged_df['ref_date_numeric'] = merged_df['ref_date'].apply(lambda x: x.timestamp() if pd.notnull(x) else None)\n",
    "merged_df['ref_date_numeric'] = pd.to_numeric(merged_df['ref_date_numeric'], errors='coerce').astype('float64')\n",
    "\n",
    "# Check the data types to confirm 'ref_date' is in datetime format\n",
    "print(merged_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the settings with specified correlation options\n",
    "config = Settings()\n",
    "config.correlations = {\n",
    "    \"pearson\": {\"calculate\": True},\n",
    "    \"spearman\": {\"calculate\": True},\n",
    "    \"kendall\": {\"calculate\": True},\n",
    "    \"phi_k\": {\"calculate\": True},\n",
    "    \"auto\": {\"calculate\": True},\n",
    "}\n",
    "\n",
    "# Generate the profile report\n",
    "profile_inter = ProfileReport(\n",
    "    merged_df,\n",
    "    title=\"EDA Report - Original Data\",\n",
    "    config=config\n",
    ")\n",
    "\n",
    "# Save the report to an HTML file\n",
    "report_filename = \"eda_report_original.html\"\n",
    "profile_inter.to_file(report_filename)\n",
    "\n",
    "# Open the report in web browser\n",
    "webbrowser.open(report_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merged_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Overview\n",
    "print(\"\\nNumeric variables:\\n\\n\",merged_df.describe())\n",
    "\n",
    "print(\"Number of obs and features:\", merged_df.shape,\"\\n\\n\")  # number of observations and features\n",
    "\n",
    "print(\"Data types:\", merged_df.dtypes,\"\\n\\n\") # data types\n",
    "\n",
    "print(\"Number of duplicated values:\", merged_df[merged_df.duplicated()],\"\\n\\n\") # check duplicated values\n",
    "\n",
    "print(\"Number of missing values per feature:\", merged_df.isna().sum(),\"\\n\\n\") # missing values per feature\n",
    "\n",
    "print(\"Number of missing values:\", merged_df.isna().sum().sum(),\"\\n\\n\") # number of missing cells\n",
    "print(\"Percentage of missing values:\", round(merged_df.isna().sum().sum() / merged_df.size * 100, 1),\"\\n\") # percentage of missing cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows based on the conditions:\n",
    "# 1. There is data in 'labour force' and a missing value in 'value_unemployment'.\n",
    "# 2. There is data in 'wages' and a missing value in 'type_of_work' AND/OR 'value_wage'.\n",
    "data_missing = merged_df[\n",
    "    ((merged_df['labour_force'].notna()) & (merged_df['value_unemployment'].isna())) |\n",
    "    ((merged_df['wages'].notna()) & (merged_df[['value_wage']].isna().any(axis=1)))\n",
    "]\n",
    "\n",
    "print(data_missing.head())\n",
    "\n",
    "# Total number of rows in the original dataset\n",
    "total_rows = merged_df.shape[0]\n",
    "\n",
    "# Count the number of rows with missing data\n",
    "missing_rows = data_missing.shape[0]\n",
    "print(missing_rows)\n",
    "\n",
    "# Calculate the percentage of rows with missing data\n",
    "missing_percentage = (missing_rows / total_rows) * 100\n",
    "\n",
    "# Output the result\n",
    "print(f\"Percentage of rows with missing data: {missing_percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize Missing Values in the DataFrame\n",
    "missing_summary = data_missing.isna().sum()\n",
    "print(\"\\nSummary of Missing Values in 'data_missing':\")\n",
    "print(missing_summary,\"\\n\\n\")\n",
    "\n",
    "# Group and Aggregate to Identify Trends\n",
    "# Grouping by 'geo', 'industry', and 'sex' for trends in missing values\n",
    "grouped_geo = data_missing.groupby('geo').size().reset_index(name='count')\n",
    "grouped_industry = data_missing.groupby('industry').size().reset_index(name='count')\n",
    "grouped_sex = data_missing.groupby('sex').size().reset_index(name='count')\n",
    "\n",
    "# Print the trends\n",
    "print(\"\\nNumber of Missing Values Grouped by 'geo':\")\n",
    "print(grouped_geo)\n",
    "\n",
    "print(\"\\n\\nNumber of Missing Values Grouped by 'industry':\")\n",
    "print(grouped_industry)\n",
    "\n",
    "print(\"\\n\\nNumber of Missing Values Grouped by 'sex':\")\n",
    "print(grouped_sex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a figure with 1 row and 3 columns with increased figure size\n",
    "fig, axes = plt.subplots(1, 3, figsize=(34, 10))\n",
    "\n",
    "# Plotting the number of missing values by 'geo'\n",
    "axes[0].bar(grouped_geo['geo'], grouped_geo['count'], color='skyblue')\n",
    "axes[0].set_xlabel('Geo')\n",
    "axes[0].set_ylabel('Number of Missing Values')\n",
    "axes[0].set_title('Number of Missing Values by Geo')\n",
    "axes[0].tick_params(axis='x', rotation=90)\n",
    "\n",
    "# Adding labels on bars for 'geo'\n",
    "for i, value in enumerate(grouped_geo['count']):\n",
    "    axes[0].text(i, value, str(value), ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "# Plotting the number of missing values by 'industry' with narrower bars for spacing\n",
    "bar_positions = np.arange(len(grouped_industry['industry']))\n",
    "bar_width = 0.3\n",
    "\n",
    "# Plotting the number of missing values by 'industry'\n",
    "axes[1].bar(grouped_industry['industry'], grouped_industry['count'], color='orange')\n",
    "axes[1].set_xlabel('Industry')\n",
    "axes[1].set_ylabel('Number of Missing Values')\n",
    "axes[1].set_title('Number of Missing Values by Industry')\n",
    "axes[1].tick_params(axis='x', rotation=90)\n",
    "axes[1].set_xticks(bar_positions)\n",
    "axes[1].set_xticklabels(grouped_industry['industry'], rotation=90)\n",
    "\n",
    "# Adding labels on bars for 'industry'\n",
    "for i, value in enumerate(grouped_industry['count']):\n",
    "    axes[1].text(i, value, str(value), ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "# Plotting the number of missing values by 'sex'\n",
    "axes[2].bar(grouped_sex['sex'], grouped_sex['count'], color='green')\n",
    "axes[2].set_xlabel('Sex')\n",
    "axes[2].set_ylabel('Number of Missing Values')\n",
    "axes[2].set_title('Number of Missing Values by Sex')\n",
    "\n",
    "# Adding labels on bars for 'sex'\n",
    "for i, value in enumerate(grouped_sex['count']):\n",
    "    axes[2].text(i, value, str(value), ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "# Adjust spacing between plots to avoid label overlap\n",
    "plt.subplots_adjust(wspace=0.4, hspace=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of value counts for all columns\n",
    "for column in data_missing.columns:\n",
    "    total_count = len(data_missing[column])\n",
    "    print(f\"\\nPercentage counts for column '{column}':\")\n",
    "    \n",
    "    # Calculate the value counts and convert them \n",
    "    percentage_counts = (data_missing[column].value_counts() / total_count) * 100\n",
    "    \n",
    "    print(percentage_counts)\n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of the data\n",
    "interpolated_data = merged_df.copy()\n",
    "\n",
    "#Unemployment Rate\n",
    "print('Unemployment Rate')\n",
    "# Apply linear interpolation\n",
    "interpolated_data['value_unemployment'] = interpolated_data['value_unemployment'].interpolate(method='linear')\n",
    "# Verify that missing values have been handled\n",
    "print(\"\\nMissing Unemployment Values in Original Data:\\n\", merged_df['value_unemployment'].isnull().sum())\n",
    "print(\"\\nMissing Unemployment Values After Interpolation in New DataFrame:\\n\", interpolated_data['value_unemployment'].isnull().sum())\n",
    "\n",
    "\n",
    "#Wage Rate\n",
    "print('\\n\\n\\nWage Rate')\n",
    "# Apply linear interpolation\n",
    "interpolated_data['value_wage'] = interpolated_data['value_wage'].interpolate(method='linear')\n",
    "# Verify that missing values have been handled\n",
    "print(\"\\nMissing Wage Values in Original Data:\\n\", merged_df['value_wage'].isnull().sum())\n",
    "print(\"\\nMissing Wage Values After Interpolation in New DataFrame:\\n\", interpolated_data['value_wage'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the settings with specified correlation options\n",
    "config = Settings()\n",
    "config.correlations = {\n",
    "    \"pearson\": {\"calculate\": True},\n",
    "    \"spearman\": {\"calculate\": True},\n",
    "    \"kendall\": {\"calculate\": True},\n",
    "    \"phi_k\": {\"calculate\": True},\n",
    "    \"auto\": {\"calculate\": True},\n",
    "}\n",
    "\n",
    "# Generate the profile report\n",
    "profile_inter = ProfileReport(\n",
    "    interpolated_data,\n",
    "    title=\"EDA Report - Interpolated Data\",\n",
    "    config=config\n",
    ")\n",
    "\n",
    "# Save the report to an HTML file\n",
    "report_filename = \"eda_report_interpolated.html\"\n",
    "profile_inter.to_file(report_filename)\n",
    "\n",
    "# Open the report in the default web browser\n",
    "webbrowser.open(report_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create copy for further analysis\n",
    "unindexed_df = interpolated_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis between unemployment and wage\n",
    "correlation_ref_date = unindexed_df.groupby(['ref_date']).agg({\n",
    "    'value_unemployment': 'mean',\n",
    "    'value_wage': 'mean'\n",
    "})\n",
    "\n",
    "correlation_year = unindexed_df.groupby(['year']).agg({\n",
    "    'value_unemployment': 'mean',\n",
    "    'value_wage': 'mean'\n",
    "})\n",
    "\n",
    "correlation_month = unindexed_df.groupby(['month']).agg({\n",
    "    'value_unemployment': 'mean',\n",
    "    'value_wage': 'mean'\n",
    "})\n",
    "\n",
    "correlation_ref_date_numeric = unindexed_df.groupby(['ref_date_numeric']).agg({\n",
    "    'value_unemployment': 'mean',\n",
    "    'value_wage': 'mean'\n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "correlation_ref_date = correlation_ref_date.corr()\n",
    "print(\"\\nCorrelation Matrix (ref_date):\\n\", correlation_ref_date )\n",
    "\n",
    "correlation_year = correlation_year.corr()\n",
    "print(\"\\n\\nCorrelation Matrix (year):\\n\", correlation_year)\n",
    "\n",
    "correlation_month = correlation_month.corr()\n",
    "print(\"\\n\\nCorrelation Matrix (month):\\n\", correlation_month)\n",
    "\n",
    "correlation_ref_date_numeric = correlation_ref_date_numeric.corr()\n",
    "print(\"\\n\\nCorrelation Matrix (ref_date_numeric):\\n\", correlation_ref_date_numeric)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the min and max value for the color scale across all correlation heatmaps\n",
    "vmin = -1  # Minimum possible value for correlation\n",
    "vmax = 1   # Maximum possible value for correlation\n",
    "\n",
    "# Create a 2x2 subplot for the correlation matrices\n",
    "fig, axes = plt.subplots(2, 2, figsize=(20, 15))\n",
    "\n",
    "# Plot heatmap for correlation matrix by 'ref_date'\n",
    "sns.heatmap(correlation_ref_date, annot=True, cmap='coolwarm', vmin=vmin, vmax=vmax, ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Correlation Matrix (ref_date)')\n",
    "axes[0, 0].set_xticklabels(correlation_ref_date.columns, rotation=45, ha='right')\n",
    "axes[0, 0].set_yticklabels(correlation_ref_date.columns, rotation=0)\n",
    "\n",
    "# Plot heatmap for correlation matrix by 'year'\n",
    "sns.heatmap(correlation_year, annot=True, cmap='coolwarm', vmin=vmin, vmax=vmax, ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Correlation Matrix (year)')\n",
    "axes[0, 1].set_xticklabels(correlation_year.columns, rotation=45, ha='right')\n",
    "axes[0, 1].set_yticklabels(correlation_year.columns, rotation=0)\n",
    "\n",
    "# Plot heatmap for correlation matrix by 'month'\n",
    "sns.heatmap(correlation_month, annot=True, cmap='coolwarm', vmin=vmin, vmax=vmax, ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Correlation Matrix (month)')\n",
    "axes[1, 0].set_xticklabels(correlation_month.columns, rotation=45, ha='right')\n",
    "axes[1, 0].set_yticklabels(correlation_month.columns, rotation=0)\n",
    "\n",
    "# Plot heatmap for correlation matrix by 'ref_date_numeric'\n",
    "sns.heatmap(correlation_ref_date_numeric, annot=True, cmap='coolwarm', vmin=vmin, vmax=vmax, ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Correlation Matrix (ref_date_numeric)')\n",
    "axes[1, 1].set_xticklabels(correlation_ref_date_numeric.columns, rotation=45, ha='right')\n",
    "axes[1, 1].set_yticklabels(correlation_ref_date_numeric.columns, rotation=0)\n",
    "\n",
    "# Adjust the layout \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normality Assumptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wage Data\n",
    "\n",
    "# Prepare the data\n",
    "print(\"\\nMedian Wage Rate\")\n",
    "column_to_test = interpolated_data['value_wage']\n",
    "\n",
    "# Conduct normality tests\n",
    "# Kolmogorov-Smirnov Test\n",
    "ks_stat, ks_p = stats.kstest(column_to_test, 'norm', args=(column_to_test.mean(), column_to_test.std()))\n",
    "print(\"\\nKolmogorov-Smirnov Test: Statistic = {:.4f}, p-value = {:.4f}\".format(ks_stat, ks_p))\n",
    "\n",
    "# Anderson-Darling Test\n",
    "anderson_result = stats.anderson(column_to_test, dist='norm')\n",
    "print(\"\\nAnderson-Darling Test: Statistic = {:.4f}\".format(anderson_result.statistic))\n",
    "print(\"Critical values:\", anderson_result.critical_values)\n",
    "print(\"Significance levels:\", anderson_result.significance_level)\n",
    "\n",
    "# D'Agostino and Pearson's Test\n",
    "dagostino_stat, dagostino_p = stats.normaltest(column_to_test)\n",
    "print(\"\\nD'Agostino and Pearson's Test: Statistic = {:.4f}, p-value = {:.4f}\".format(dagostino_stat, dagostino_p))\n",
    "\n",
    "# Visualizations\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Histogram\n",
    "plt.subplot(1, 3, 1)\n",
    "sns.histplot(column_to_test, kde=True)\n",
    "plt.title(\"Histogram\")\n",
    "\n",
    "# QQ-Plot\n",
    "plt.subplot(1, 3, 2)\n",
    "stats.probplot(column_to_test, dist=\"norm\", plot=plt)\n",
    "plt.title(\"QQ-Plot\")\n",
    "\n",
    "# Box Plot\n",
    "plt.subplot(1, 3, 3)\n",
    "sns.boxplot(x=column_to_test)\n",
    "plt.title(\"Box Plot\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unemployment Data\n",
    "\n",
    "# Prepare the data\n",
    "print(\"\\nUnemployment Rate\")\n",
    "column_to_test_emp = interpolated_data['value_unemployment']\n",
    "\n",
    "# Conduct normality tests\n",
    "\n",
    "# Kolmogorov-Smirnov Test\n",
    "ks_stat_emp, ks_p_emp = stats.kstest(column_to_test_emp, 'norm', args=(column_to_test_emp.mean(), column_to_test_emp.std()))\n",
    "print(\"\\nKolmogorov-Smirnov Test: Statistic = {:.4f}, p-value = {:.4f}\".format(ks_stat_emp, ks_p_emp))\n",
    "\n",
    "# Anderson-Darling Test\n",
    "anderson_result_emp = stats.anderson(column_to_test_emp, dist='norm')\n",
    "print(\"\\nAnderson-Darling Test: Statistic = {:.4f}\".format(anderson_result_emp.statistic))\n",
    "print(\"Critical values:\", anderson_result_emp.critical_values)\n",
    "print(\"Significance levels:\", anderson_result_emp.significance_level)\n",
    "\n",
    "# D'Agostino and Pearson's Test\n",
    "dagostino_stat_emp, dagostino_p_emp = stats.normaltest(column_to_test_emp)\n",
    "print(\"\\nD'Agostino and Pearson's Test: Statistic = {:.4f}, p-value = {:.4f}\".format(dagostino_stat_emp, dagostino_p_emp))\n",
    "\n",
    "# Visualizations\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Histogram\n",
    "plt.subplot(1, 3, 1)\n",
    "sns.histplot(column_to_test_emp, kde=True)\n",
    "plt.title(\"Histogram\")\n",
    "\n",
    "# QQ-Plot\n",
    "plt.subplot(1, 3, 2)\n",
    "stats.probplot(column_to_test_emp, dist=\"norm\", plot=plt)\n",
    "plt.title(\"QQ-Plot\")\n",
    "\n",
    "# Box Plot\n",
    "plt.subplot(1, 3, 3)\n",
    "sns.boxplot(x=column_to_test_emp)\n",
    "plt.title(\"Box Plot\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy dataframe\n",
    "plot_df = merged_df.copy()\n",
    "\n",
    "# Ensure 'ref_date' is in datetime format\n",
    "plot_df['ref_date'] = pd.to_datetime(plot_df['ref_date'], errors='coerce')\n",
    "\n",
    "# Set 'ref_date' as the index to enable resampling\n",
    "plot_df.set_index('ref_date', inplace=True)\n",
    "\n",
    "# Drop any rows where 'ref_date' could not be converted to datetime (in case some were NaT)\n",
    "plot_df.dropna(subset=['value_unemployment'], inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# Create figure \n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 12))  \n",
    "\n",
    "# Set the overall title\n",
    "fig.suptitle('Original Data', fontsize=16)\n",
    "\n",
    "# Define common x-axis range based on the min and max index values\n",
    "x_min, x_max = plot_df.index.min(), plot_df.index.max()\n",
    "\n",
    "# Set common y-axis limit\n",
    "y_max = 30\n",
    "\n",
    "# Resample the unemployment rate to get the monthly average and plot\n",
    "df_monthly_emp = plot_df['value_unemployment'].resample('ME').mean()\n",
    "axes[0, 0].plot(df_monthly_emp.index, df_monthly_emp.values, color='blue', linewidth=1)\n",
    "axes[0, 0].set_title('Average Monthly Unemployment Rate Time Series')\n",
    "axes[0, 0].set_xlabel('Date')\n",
    "axes[0, 0].set_ylabel('Unemployment Rate')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "axes[0, 0].grid(True)\n",
    "axes[0, 0].set_xlim([x_min, x_max])\n",
    "axes[0, 0].set_ylim([0, y_max])\n",
    "\n",
    "# Resample the unemployment rate to get the yearly average and plot\n",
    "df_yearly_emp = plot_df['value_unemployment'].resample('YE').mean()\n",
    "axes[0, 1].plot(df_yearly_emp.index, df_yearly_emp.values, color='blue', linewidth=1)\n",
    "axes[0, 1].set_title('Average Yearly Unemployment Rate Time Series')\n",
    "axes[0, 1].set_xlabel('Date')\n",
    "axes[0, 1].set_ylabel('Unemployment Rate')\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "axes[0, 1].grid(True)\n",
    "axes[0, 1].set_xlim([x_min, x_max])\n",
    "axes[0, 1].set_ylim([0, y_max])\n",
    "\n",
    "# Resample the wage rate to get the monthly average and plot\n",
    "df_monthly_wage = plot_df['value_wage'].resample('ME').mean()\n",
    "axes[1, 0].plot(df_monthly_wage.index, df_monthly_wage.values, color='orange', linewidth=1)\n",
    "axes[1, 0].set_title('Average Monthly Median Wage Rate Time Series')\n",
    "axes[1, 0].set_xlabel('Date')\n",
    "axes[1, 0].set_ylabel('Median Wage Rate')\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "axes[1, 0].grid(True)\n",
    "axes[1, 0].set_xlim([x_min, x_max])\n",
    "axes[1, 0].set_ylim([0, y_max])\n",
    "\n",
    "# Resample the wage rate to get the yearly average and plot\n",
    "df_yearly_wage = plot_df['value_wage'].resample('YE').mean()\n",
    "axes[1, 1].plot(df_yearly_wage.index, df_yearly_wage.values, color='orange', linewidth=1)\n",
    "axes[1, 1].set_title('Average Yearly Median Wage Rate Time Series')\n",
    "axes[1, 1].set_xlabel('Date')\n",
    "axes[1, 1].set_ylabel('Median Wage Rate')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "axes[1, 1].grid(True)\n",
    "axes[1, 1].set_xlim([x_min, x_max])\n",
    "axes[1, 1].set_ylim([0, y_max])\n",
    "\n",
    "# Adjust layout to ensure subplots don't overlap\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy dataframe\n",
    "plot_interpolated = interpolated_data.copy()\n",
    "\n",
    "# Ensure 'ref_date' is in datetime format\n",
    "plot_interpolated['ref_date'] = pd.to_datetime(plot_interpolated['ref_date'], errors='coerce')\n",
    "\n",
    "# Set 'ref_date' as the index to enable resampling\n",
    "plot_interpolated.set_index('ref_date', inplace=True)\n",
    "\n",
    "# Drop any rows where 'ref_date' could not be converted to datetime (in case some were NaT)\n",
    "plot_interpolated.dropna(subset=['value_unemployment'], inplace=True)\n",
    "\n",
    "# Create figure \n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 12))  \n",
    "\n",
    "# Set the overall title\n",
    "fig.suptitle('Cleaned Data', fontsize=16)\n",
    "\n",
    "# Define common x-axis range based on the min and max index values\n",
    "x_min, x_max = plot_interpolated.index.min(), plot_interpolated.index.max()\n",
    "\n",
    "# Set common y-axis limit\n",
    "y_max = 30\n",
    "\n",
    "# Resample the unemployment rate to get the monthly average and plot\n",
    "df_monthly_emp = plot_interpolated['value_unemployment'].resample('ME').mean()\n",
    "axes[0, 0].plot(df_monthly_emp.index, df_monthly_emp.values, color='blue', linewidth=1)\n",
    "axes[0, 0].set_title('Average Monthly Unemployment Rate Time Series')\n",
    "axes[0, 0].set_xlabel('Date')\n",
    "axes[0, 0].set_ylabel('Unemployment Rate')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "axes[0, 0].grid(True)\n",
    "axes[0, 0].set_xlim([x_min, x_max])\n",
    "axes[0, 0].set_ylim([0, y_max])\n",
    "\n",
    "# Resample the unemployment rate to get the yearly average and plot\n",
    "df_yearly_emp = plot_interpolated['value_unemployment'].resample('YE').mean()\n",
    "axes[0, 1].plot(df_yearly_emp.index, df_yearly_emp.values, color='blue', linewidth=1)\n",
    "axes[0, 1].set_title('Average Yearly Unemployment Rate Time Series')\n",
    "axes[0, 1].set_xlabel('Date')\n",
    "axes[0, 1].set_ylabel('Unemployment Rate')\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "axes[0, 1].grid(True)\n",
    "axes[0, 1].set_xlim([x_min, x_max])\n",
    "axes[0, 1].set_ylim([0, y_max])\n",
    "\n",
    "# Resample the wage rate to get the monthly average and plot\n",
    "df_monthly_wage = plot_interpolated['value_wage'].resample('ME').mean()\n",
    "axes[1, 0].plot(df_monthly_wage.index, df_monthly_wage.values, color='orange', linewidth=1)\n",
    "axes[1, 0].set_title('Average Monthly Median Wage Rate Time Series')\n",
    "axes[1, 0].set_xlabel('Date')\n",
    "axes[1, 0].set_ylabel('Median Wage Rate')\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "axes[1, 0].grid(True)\n",
    "axes[1, 0].set_xlim([x_min, x_max])\n",
    "axes[1, 0].set_ylim([0, y_max])\n",
    "\n",
    "# Resample the wage rate to get the yearly average and plot\n",
    "df_yearly_wage = plot_interpolated['value_wage'].resample('YE').mean()\n",
    "axes[1, 1].plot(df_yearly_wage.index, df_yearly_wage.values, color='orange', linewidth=1)\n",
    "axes[1, 1].set_title('Average Yearly Median Wage Rate Time Series')\n",
    "axes[1, 1].set_xlabel('Date')\n",
    "axes[1, 1].set_ylabel('Median Wage Rate')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "axes[1, 1].grid(True)\n",
    "axes[1, 1].set_xlim([x_min, x_max])\n",
    "axes[1, 1].set_ylim([0, y_max])\n",
    "\n",
    "# Adjust layout to ensure subplots don't overlap\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a figure with two subplots side by side\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Plot the unemployment rate distribution on the first subplot\n",
    "sns.histplot(interpolated_data['value_unemployment'], kde=True, bins=20, color='blue', ax=axes[0])\n",
    "axes[0].set_title('Distribution of Unemployment Rate')\n",
    "axes[0].set_xlabel('Unemployment Rate')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "\n",
    "# Plot the average hourly wage distribution on the second subplot\n",
    "sns.histplot(interpolated_data['value_wage'], kde=True, bins=20, color='orange', ax=axes[1])\n",
    "axes[1].set_title('Distribution of Median Hourly Wage')\n",
    "axes[1].set_xlabel('Median Hourly Wage')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "\n",
    "# Adjust layout to ensure there is no overlap between subplots\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot distributions of unemployment rate and wages on the same plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot unemployment rate distribution\n",
    "sns.histplot(interpolated_data['value_unemployment'], kde=True, bins=20, color='blue', label='Unemployment Rate')\n",
    "\n",
    "# Plot average hourly wage distribution\n",
    "sns.histplot(interpolated_data['value_wage'], kde=True, bins=20, color='orange', label='Median Hourly Wage')\n",
    "\n",
    "# Add titles, labels, and legend\n",
    "plt.title('Distribution of Unemployment Rate and Median Hourly Wage')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend(title='Legend')  # Add a legend with a title\n",
    "\n",
    "# Adjust layout to avoid overlap\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stationarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADF - Unemployment "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform ADF test on 'value' column to check for stationarity\n",
    "adf_test = adfuller(interpolated_data['value_unemployment'])\n",
    "print(\"\\nADF Test Results:\")\n",
    "print(f\"ADF Statistic: {adf_test[0]}\")\n",
    "print(f\"p-value: {adf_test[1]}\")\n",
    "if adf_test[1] < 0.05:\n",
    "    print(\"The data is stationary.\")\n",
    "else:\n",
    "    print(\"The data is not stationary. Differencing may be required.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot ACF and PACF to determine p and q\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(121)\n",
    "plot_acf(interpolated_data['value_diff'].dropna() if 'value_diff' in interpolated_data else interpolated_data['value_unemployment'], \n",
    "         ax=plt.gca(), lags=40)\n",
    "plt.title(\"ACF Plot\")\n",
    "plt.subplot(122)\n",
    "plot_pacf(interpolated_data['value_diff'].dropna() if 'value_diff' in interpolated_data else interpolated_data['value_unemployment'], \n",
    "          ax=plt.gca(), lags=40)\n",
    "plt.title(\"PACF Plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADF - Wage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform ADF test on 'value' column to check for stationarity\n",
    "adf_test = adfuller(interpolated_data['value_wage'])\n",
    "print(\"\\nADF Test Results:\")\n",
    "print(f\"ADF Statistic: {adf_test[0]}\")\n",
    "print(f\"p-value: {adf_test[1]}\")\n",
    "if adf_test[1] < 0.05:\n",
    "    print(\"The data is stationary.\")\n",
    "else:\n",
    "    print(\"The data is not stationary. Differencing may be required.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot ACF and PACF to determine p and q\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(121)\n",
    "plot_acf(interpolated_data['value_diff'].dropna() if 'value_diff' in interpolated_data else interpolated_data['value_wage'], \n",
    "         ax=plt.gca(), lags=40)\n",
    "plt.title(\"ACF Plot\")\n",
    "plt.subplot(122)\n",
    "plot_pacf(interpolated_data['value_diff'].dropna() if 'value_diff' in interpolated_data else interpolated_data['value_wage'], \n",
    "          ax=plt.gca(), lags=40)\n",
    "plt.title(\"PACF Plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group data by reference date to create a time series of unemployment rate and average hourly wage\n",
    "time_series_df = interpolated_data.groupby(['ref_date']).agg({\n",
    "    'value_unemployment': 'mean',\n",
    "    'value_wage': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "# Plot unemployment rate and wages over time\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(time_series_df['ref_date'], time_series_df['value_unemployment'], label='Unemployment Rate')\n",
    "plt.plot(time_series_df['ref_date'], time_series_df['value_wage'], label='Average Hourly Wage', color='orange')\n",
    "plt.title('Unemployment Rate and Average Hourly Wage Over Time')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
